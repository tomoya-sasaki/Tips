{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [here](https://null-byte.wonderhowto.com/how-to/download-all-pdfs-webpage-with-python-script-0163031/)\n",
    "* [here](https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as urllib\n",
    "import urllib3\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllibR\n",
    "from requests import get\n",
    "from time import sleep, time\n",
    "import re\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to download (bypass using firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def obtain_bs4soup(url, bypass):\n",
    "    try:\n",
    "        os.mkdir(download_path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "    }\n",
    "    \n",
    "    ## Choose whether to bypass through firefox\n",
    "    if bypass == True:\n",
    "        request0 = urllibR.Request(url=url, headers=headers)\n",
    "        request = urllibR.urlopen(request0)\n",
    "    else:\n",
    "        request = urllibR.urlopen(url)\n",
    "        \n",
    "    soup = BeautifulSoup(request.read(), \"lxml\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def download(url, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        try:\n",
    "            # get request\n",
    "            response = get(url)\n",
    "            # write to file\n",
    "            file.write(response.content)\n",
    "        except:\n",
    "            print(\"fail\", file_name)\n",
    "#         print(\"wrote\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you get the following error, use this. \n",
    "## There is some problem in the SSL \n",
    "## <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed ... >\n",
    "## https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error\n",
    "## https://shinespark.hatenablog.com/entry/2015/12/06/100000\n",
    "import ssl\n",
    "\n",
    "# This restores the same behavior as before.\n",
    "context = ssl._create_unverified_context()\n",
    "urllib.urlopen(\"https://no-valid-cert\", context=context)\n",
    "\n",
    "# more discouraged options\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "url = \"https://web.stanford.edu/~imalone/Teaching/ps1.html\"\n",
    "# url = \"https://projects.iq.harvard.edu/prefresher/math\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# download_path = \"/Users/tomoyasasaki/Documents/Materials/Lectures/AdvancedStats_Soc504Brandon\"\n",
    "download_path = \"/Users/tomoyasasaki/Documents/Materials/Lectures/Politicsintro_stanford_ps1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "soup = obtain_bs4soup(url, bypass = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS1%20Makeup%20Assignment%20Prompt.pdf\n",
      "PS_1_Section_Syllabus_Winter17.pdf\n",
      "PS1-Week1Recap.pdf\n",
      "AnalyzePoliSciData.pdf\n",
      "ApplicationRoss2001.pdf\n",
      "PS1-Week2Recap.pdf\n",
      "MilitaryJuntaGameRules.pdf\n",
      "gambia-yahya-jammeh-adama-barrow.html\n",
      "PS1-Week3Recap.pdf\n",
      "PS1-DataAnalysisAssignmentCheatSheet.pdf\n",
      "PS1-Week4Recap.pdf\n",
      "PS1-Week5Recap.pdf\n",
      "ForeignAidDebateRules.pdf\n",
      "PS1-Week7Recap.pdf\n",
      "ClimateChangeOpinion.pdf\n",
      "PS1-Week7Recap.pdf\n",
      "PS1-EnvironmentalPolicy.pdf\n",
      "california-retains-drought-measures-despite-wet-weather.html\n",
      "PS1-Week8Recap.pdf\n",
      "PS1-PolicyMemo.pdf\n",
      "PS1-CheapTalk.pdf\n",
      "north-korea-nuclear-threat.html\n",
      "pakistan-must-vacate-illegal-occupation-of-kashmir-india-at-united-nations_1982559.html\n",
      "IraqsSectarianCrisis_0.pdf\n",
      "PS1-Week9Recap.pdf\n",
      "PS1-Week10Recap.pdf\n",
      "PS1-FinalRecap.pdf\n",
      "\n",
      " END\n"
     ]
    }
   ],
   "source": [
    "## if the URL for the target file is straight forward, use this\n",
    "## e.g. <a href=\"/path/to/file.pdf\">\n",
    "\n",
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "    if os.path.splitext(os.path.basename(tag2))[1] == \".pdf\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".ipynb\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".py\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".tex\" or\\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".zip\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".ppt\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".RData\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".html\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".R\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".txt\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".Rmd\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".md\" or \\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".r\" or\\\n",
    "    os.path.splitext(os.path.basename(tag2))[1] == \".csv\":\n",
    "\n",
    "#     if len( os.path.splitext(os.path.basename(tag2))[1]  ) >= 1:\n",
    "        name = os.path.basename(tag2)\n",
    "#         name = os.path.basename(tag2)[:-2]\n",
    "        download(tag2, name)\n",
    "        print(name)\n",
    "        sleep(2)\n",
    "\n",
    "print(\"\\n END\")\n",
    "# end = time()\n",
    "# elapse = end - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics1_2017\n",
      "metrics2_2017\n",
      "metrics3_2017\n",
      "metrics4_2017\n",
      "metrics5_2017\n",
      "metrics6_2017\n"
     ]
    }
   ],
   "source": [
    "## if the URL for the target file is NOT straight forward, use this\n",
    "## e.g. <a href=\"path/to/file.pdf?attredirects=0&amp;d=1\">\n",
    "\n",
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "    if \".pdf\" in os.path.splitext(os.path.basename(tag2))[1]:\n",
    "        name = os.path.splitext(os.path.basename(tag2))[0]\n",
    "        download(tag2, name + \".pdf\")\n",
    "        print(name)\n",
    "        sleep(1)\n",
    "\n",
    "# end = time()\n",
    "# elapse = end - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch\n",
    "\n",
    "os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "#     print(os.path.splitext(os.path.basename(tag['href'])) )\n",
    "#     print(tag2)\n",
    "#     if os.path.splitext(os.path.basename(tag2))[1] == \".pdf\" or os.path.splitext(os.path.basename(tag2))[1] == \".r\"\\\n",
    "    if \".xls\" in os.path.splitext(os.path.basename(tag2))[1]:\n",
    "#         print(tag2)\n",
    "#         name = os.path.basename(tag2)[:-2]\n",
    "        name = os.path.splitext(os.path.basename(tag2))[0]\n",
    "#         download(tag2, name)\n",
    "        download(tag2, name + \".xls\")\n",
    "        print(name)\n",
    "#         if name == \"sig_phrases_det.pdf\":\n",
    "#             pass\n",
    "#         else:\n",
    "#         download(tag2, re.sub(r\"\\?.+\" ,\"\",os.path.basename(tag2) ))\n",
    "#         download(url + name, name)\n",
    "#         sleep(1)\n",
    "#         tmp.append(tag2)\n",
    "\n",
    "# end = time()\n",
    "# elapse = end - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blackboard.princeton.edu/webapps/login/?new_loc=%2Fwebapps%2Fblackboard%2Fcontent%2FlistContent.jsp%3Fcourse_id%3D_6116604_1%26content_id%3D_2189650_1%26mode%3Dreset\n"
     ]
    }
   ],
   "source": [
    "# os.chdir(download_path)\n",
    "for tag in soup.findAll('a', href = True):\n",
    "    tag2 = urllibR.urljoin(url, tag['href'])\n",
    "    print(tag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"CY2018.csv\">Export Grain Inspection 2018 (last updated 4/23/2018 7:25:28 AM)</a>,\n",
       " <a href=\"CY2017.csv\">Export Grain Inspection 2017 (last updated 2/5/2018 9:31:27 AM)</a>,\n",
       " <a href=\"CY2016.csv\">Export Grain Inspection 2016 (last updated 9/18/2017 7:58:29 AM)</a>,\n",
       " <a href=\"CY2015.csv\">Export Grain Inspection 2015 (last updated 11/7/2016 6:33:34 AM)</a>,\n",
       " <a href=\"CY2014.csv\">Export Grain Inspection 2014 (last updated 5/26/2015 4:34:21 PM)</a>,\n",
       " <a href=\"CY2013.csv\">Export Grain Inspection 2013 (last updated 6/20/2014 8:22:09 AM)</a>,\n",
       " <a href=\"CY2012.csv\">Export Grain Inspection 2012 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2011.csv\">Export Grain Inspection 2011 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2010.csv\">Export Grain Inspection 2010 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2009.csv\">Export Grain Inspection 2009 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2008.csv\">Export Grain Inspection 2008 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2007.csv\">Export Grain Inspection 2007 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2006.csv\">Export Grain Inspection 2006 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2005.csv\">Export Grain Inspection 2005 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2004.csv\">Export Grain Inspection 2004 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2003.csv\">Export Grain Inspection 2003 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2002.csv\">Export Grain Inspection 2002 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2001.csv\">Export Grain Inspection 2001 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY2000.csv\">Export Grain Inspection 2000 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1999.csv\">Export Grain Inspection 1999 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1998.csv\">Export Grain Inspection 1998 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1997.csv\">Export Grain Inspection 1997 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1996.csv\">Export Grain Inspection 1996 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1995.csv\">Export Grain Inspection 1995 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1994.csv\">Export Grain Inspection 1994 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1993.csv\">Export Grain Inspection 1993 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1992.csv\">Export Grain Inspection 1992 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1991.csv\">Export Grain Inspection 1991 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1990.csv\">Export Grain Inspection 1990 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1989.csv\">Export Grain Inspection 1989 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1988.csv\">Export Grain Inspection 1988 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1987.csv\">Export Grain Inspection 1987 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1986.csv\">Export Grain Inspection 1986 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1985.csv\">Export Grain Inspection 1985 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1984.csv\">Export Grain Inspection 1984 (last updated 4/8/2013 12:00:00 AM)</a>,\n",
       " <a href=\"CY1983.csv\">Export Grain Inspection 1983 (last updated 4/8/2013 12:00:00 AM)</a>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('a', href = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " END\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
